<h2 align="center">09 Dimensionality Reduction: PCA, LDA and tSNE</h2>

## Material:
Ch 8

[Session material](https://viaucdk-my.sharepoint.com/:f:/g/personal/rib_viauc_dk/EtRvmMnYXORCi6BvOYlcc5IBsurTuKp_Tj7q-MY-SjV4ng?e=ttcO3r)

[Steve Brunton](https://youtube.com/playlist?list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv&si=q7-RepDmv-fnb5PH) has made a whole lecture series about the SVD. This is overkill but maybe check out the the Overview and the videoes about PCA.

For a very appealing and visual explanation of SVD, you should take a look at [Visual Kernel](https://www.youtube.com/watch?v=vSczTbgc8Rc&list=PLWhu9osGd2dB9uMG5gKBARmk73oHUUQZS&index=4)'s video on the topic.

And as always, Alexander Ihler is gold.


## Topics
This lecture covers **unsupervised machine learning** algorithms. We discuss how these can be used for dimensionality reduction. We cover the following algorithms:

- Principal component analysis (PCA)
- Linear discriminant analysis (LDA),
- t-distributed stochastic neighbor embedding (t-SNE)

After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:

- Use principle component analysis (PCA) to reduce the dimensions of your dataset
- Describe how PCA can be used for clustering analyses
- Create 2-dimensional clustering-plots in python using PCA and t-SNE

If you missed the session in linear algebra, I recommend checking out some of the resources mentioned above.
