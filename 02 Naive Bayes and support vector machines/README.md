<h2 align="center">02 Classification - Naïve Bayes & Support vector machine</h2>

## Material:
Ch 5 (Up to and including "Nonlinear SVM Classification", pp. 147 - 157) + [Naïve Bayes](https://www.ibm.com/topics/naive-bayes)

[Session material](https://viaucdk-my.sharepoint.com/:f:/g/personal/rib_viauc_dk/Eh1UjeNInrFNr58q-MtfviEBCtIQTzYGV6ez-AMGf_l8GQ?e=CEW9TZ)

[Session notes (Monday)](https://drive.google.com/file/d/1b8-u6oY8FHurOY4OYRYF6BbIRmhz9XNl/view?usp=sharing)

[Session notes (Tuesday)](https://drive.google.com/file/d/1J-_dAEpwynMV2PzUnUJGHhvDFE4eBim9/view?usp=sharing)



## Topics
This lecture will delve into more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM) and Naive Bayes classifiers.

After attending this lecture and reading the corresponding part of the book, I expect you to be able to:

- Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.
- Explain and exemplify how adding new features can make non-linearly separable data linearly separable.
- Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.
- Discuss how, when using a Gaussian kernel, the hyperparameters C and γ influence the decision boundaries.
- Discuss advantages and disadvantages of support vector machines.
- Understand the probabilistic foundations of the Naive Bayes classifier and its assumptions about feature independence.
- Describe the different types of Naive Bayes classifiers (e.g., Gaussian, Multinomial) and their appropriate application contexts.
- Apply a Naive Bayes classifier to text data and other types of datasets using sklearn.
- Discuss the concept of hyperparameters in the context of SVM and Naive Bayes, and demonstrate how to tune them to improve model performance.
