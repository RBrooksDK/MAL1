{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explore several performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (train_test_split, cross_val_predict, \n",
    "                                     GridSearchCV)\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,auc,\n",
    "                             f1_score, precision_recall_curve,accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples = 1000, noise = 0.45, random_state = 42)\n",
    "plt.figure(dpi=800)\n",
    "mglearn.discrete_scatter(X[:,0], X[:,1], y)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the performance of an SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the least, you need to be aware of the different metrics and investigate them all to see that, for example, you don't have a low recall you would have otherwise missed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sklearn documentation: \n",
    "\n",
    "\"By definition a confusion matrix C is such that C(i,j) is equal to the number of observations known to be in group i and predicted to be in group j.\n",
    "\n",
    "Thus in binary classification, the count of true negatives is C(0,0), false negatives is C(1,0), true positives is C(1,1) and false positives is C(0,1).\"\n",
    "\n",
    "So the structure of confusion matrix shown above is\n",
    "\n",
    "    [[TN FP]\n",
    "\n",
    "     [FN TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then plot this curve using a range of different models and see which one's got the highest AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing the model to higher recall or precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in detail at this threshold business. For SVM, the default threshold is 0.5 - when the models thinks there's more than a 50 % chance it's positive, it will be classified as positive. But we could force the model to change the threshold to avoid either false negatives or false positives. Now we're really going into the machine room and changing the fundamental assumptions of the algorithms - so you've got to be careful and know what you're doing! Let's say we're going for 95 % recall - at the expense of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, rather than 50 %, we classify as positive as soon as we are more than 25 % certain it's a positive. If there's a 25 % chance we're dealing with a terrorist, we probably want to take an extra look at the person!\n",
    "\n",
    "So what we have to do now is use the *probabilities* predicted by our SVM classifier and then build a new classifier with a threshold of 25 %:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we could force our model to 95 % precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note we use argmax here, not argmin - the recall and precision lists go in opposite orders\n",
    "idx_for_95_precision = (prec > 0.95).argmax()\n",
    "threshold_for_95_precision = thresholds[idx_for_95_precision]\n",
    "threshold_for_95_precision\n",
    "predictions_95_prec = (clf.predict_proba(X_test)[:,1] >= threshold_for_95_precision)\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_test, predictions_95_prec)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_test, predictions_95_prec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replot the precision-recall curve to see where we've moved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec,rec,thresholds = precision_recall_curve(y_test,\n",
    "                                             clf.predict_proba(X_test)[:,1])\n",
    "plt.figure(dpi=800)\n",
    "plt.plot(rec,prec,'b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.plot(recall_score(y_test,predictions),\n",
    "         precision_score(y_test,predictions),'bo', label = \"Standard 50/50 threshold\")\n",
    "plt.plot(recall_score(y_test, predictions_95_rec),\n",
    "         precision_score(y_test, predictions_95_rec),'ro', label = \"Forced to 95 % recall\")\n",
    "plt.plot(recall_score(y_test, predictions_95_prec),\n",
    "         precision_score(y_test, predictions_95_prec),'go', label = \"Forced to 95 % precision\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it's rarely seen, we could explicitly plot the recall and precision against the threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 800)\n",
    "plt.plot(thresholds, prec[:-1], \"b-\", linewidth = 2)\n",
    "plt.plot(thresholds, rec[:-1], \"r-\", linewidth = 2)\n",
    "\n",
    "plt.axvline(threshold_for_95_precision, color = 'k', linestyle = '--')\n",
    "plt.text(threshold_for_95_precision-0.03,0,'Threshold for 95 % precision',rotation=90)\n",
    "plt.axvline(0.5, color = 'k', linestyle = '--')\n",
    "plt.text(0.47,0,'Default threshold',rotation=90)\n",
    "plt.axvline(threshold_for_95_recall, color = 'k', linestyle = '--')\n",
    "plt.text(threshold_for_95_recall-0.03,0,'Threshold for 95 % recall',rotation=90)\n",
    "\n",
    "plt.text(0.1,0.915,'Precision',color = 'r')\n",
    "plt.text(0.1,0.79,'Recall',color = 'b')\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
