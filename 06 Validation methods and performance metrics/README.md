<h2 align="center">06 Validation Methods and Performance Metrics</h2>

## Material:

Ch 3

Session material: In this folder


## Topics
This lecture will cover several machine learning methodologies:

- The train/test-methodelogy
- The validation set methodology
- The cross-validation methodology
- The leave-one-out-methodology

In addition to this, we will discuss what is meant by true/false positives/negatives, and explore alternative performance metrics (so far, we have only encountered the “accuracy”-metric):

- Accuracy
- Confusion matrix
- Recall
- Precision
- F1-score
- Precision-recall-curve

After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:

- Describe the "validation set"-methodology
- Describe the "cross validation"-methodology
- Describe the "leave one out"-methodology
- Apply each of the 3 methodologies above in sklearn
- Do hyperparameter tuning in sklearn using each of the 3 methodologies above (e.g. using the GridSearchCV-function in sklearn)
- Explain and calculate (in python) the following performance metrics for supervised classification:
  - Confusion matrix
  - Accuracy
  - Recall ( = "True Positive Rate" (TPR) )
  - Precision ( = "Positive Prediction Rate" (PPR) )
  - F1-score
  - Precision-recall-curve

**Useful resources:**

[Performance Metrics for Classification Problems](https://www.kaggle.com/code/usengecoder/performance-metrics-for-classification-problems)

[Performance Metrics in Machine Learning](https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide)

[24 Evaluation Metrics for Binary Classification](https://neptune.ai/blog/evaluation-metrics-binary-classification)
