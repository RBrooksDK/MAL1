<h2 align="center">08 Regression</h2>

## Material:

Ch 4 (except "Logistic Regression")

These are some useful resources for some of the main concepts:

[Linear regression](https://youtu.be/hiOQDsdOZ7I?si=kxiM6QgQa5C2LQd4), [Regularization](https://www.youtube.com/watch?v=sO4ZirJh9ds)

In general we recommend Alexander Ihler’s course [“Machine Learning and Data Mining”](https://youtube.com/playlist?list=PLaXDtXvwY-oDvedS3f4HW0b4KxqpJ_imw&si=60z8GWh1ks6OWyag).

Session material: In this folder.


## Topics
Today, we are going to look at regression algorithms, where instead of predicting a class you predict some continuous variable.

We mainly focus on linear regression algorithms:

- Ordinary Least Squares (OLS) regression
- Ridge regression
- Lasso regression

We also discuss (again) what is meant by “regularization” and consider the R² performance metric for regression. Last but not least we will introduce one of the most important concepts in Machine Learning: Gradient Descent.

After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:

- Explain what is meant by "regression" and in which contexts to apply it
- Explain the following linear regression models, their strengths and weaknesses, and apply them in python:
  - Ordinary Least Squares (OLS) regression
  - Ridge regression
  - Lasso regression
  - Elastic Net Regression
- Explain what is meant by the term "regularization" in an ML-context
- Describe what is meant by "bias" and "variance" in relation to ML-algorithms
- Explain the R²-metric for evaluating the performance of a linear regression algorithm
- Explain the MSE metric
- Explain how regression models can be trained

